.. _rnn_primer:

Primer on Recurrent Neural Networks
===================================
.. Note::
    This tutorial assumes some familarity with recurrent neural models and
    it is meant more as a refresher on this model. At the end of the tutorial we provide a
    set of papers that could be used gain more insight into RNNs.

A recurrent neural network (RNN) is a recursive map, that at time step :math:`t` combines the state at
:math:`t-1` with the input to construct a new state. In order to formalize this sentence let us define
some notation:

  * :math:`u` stands for the input sequence. :math:`u(t)` is the frame at position :math:`t` in the sequence of inputs

  * :math:`h` stands for the (hidden) state of the model. :math:`h(t)` is the hidden state at step `t`

  * :math:`y` is the output of the model. :math:`y(t)` is the output at time step `t`

  * :math:`W_{hh}` is the recurrent weight matrix (that connects the hidden state :math:`h` to itself)

  * :math:`W_{uh}` is the input weight matrix that goes from the input :math:`u` to the hidden state :math:`h`

  * :math:`W_{hy}` is the ouput weight matrix that goes from the hidden state :math:`h` to the ouput layer :math:`y`

  * :math:`b_h` is the hidden state bias 

  * :math:`b_y` is the output layer bias

  * :math:`\sigma_h` is the activation function for the hidden state

  * :math:`\sigma_y` is the activation function for the output layer

The RNN is defined by the following two equations.

.. math::

    h(t) = f(u(t), h(t-1)) = \sigma_h(h(t-1)W_{hh} + u(t) W_{uh} + b_h)

    y(t) = \sigma_y(h(t) W_{hy} + b_y)

The recurrence is started 
from :math:`h_0` (the initial hidden state) which is provided by the practitioner 
or set to zero. At each time step we compute a new state :math:`h_t` and
predict an output for that step :math:`y_t`. Below this is represented as a
diagram:

.. figure:: images/rnn.png
    :align: center

.. Note::
    The RNN is similar to an MLP, with the exception that we allow
    connections among the hidden units. This changes drastically the model.
    Through these recurrent connections
    the model exhibits ''memory''. A RNN can approximate arbitrarly well any
    dynamical system, compared to a feed-forward neural model that can only
    approximate nonlinear functions.

The model can be used in several ways. One standard approach is to use 
the RNN to transcribe some input sequence into an output sequence. 
This means that the output sequence is of the same length as the input one, 
and at each time step the ouput of the RNN :math:`y(t)` is the prediction of
the :math:`t`-th frame of the target sequence. 

Another task could be to classify the possible input sequences into
different categories. One approach is to only consider the output at the end
of the sequence (after the model has seen the whole sequence). If the input 
sequence :math:`u` has :math:`T` steps, then :math:`y(T)` could be a softmax 
layer saying in which the category the current sequence falls. 

Training Recurrent Neural Networks
----------------------------------

Several algorithms are available for training recurrent neural networks. 
Real Time Recurrent Learning (RTRL), Atyia-Paros learning rule are just a 
few example. An interesting different approach which involves using the recurrent neural 
network to construct a random projection has also been proposed under the
name of Reservoir Computing.  Long Short Term Memory networks are recurrent 
neural networks that have a specific structure in the hidden layer which
helps them deal with long term correlations.

Backpropagation Through Time (BPTT)
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

BPTT is a widely used algorithm for computing the gradients of the recurrent
neural model. The idea is to unfold the model in time, resulting in a
potentially unbounded number of layers. In the figure below we show three
time steps corresponding to three layers (on the horizontal). :math:`C(t)`
stands for the cost at time step :math:`t`. 

.. figure:: images/unfolded.png
    :align: center

The green and blue arrors moving backwards in time shows the flow of the
gradient if one simply applies backpropagation on the unfolded graph. 

The following snipped of code sketches how one can implement efficiently
BPTT (note that before computing the gradients one has to do a forward pass
to compute all activations):

.. figure:: images/rnn_bptt.png
    :align: center
    :scale: 60



Difficulties of training recurrent models
-----------------------------------------

Besides the standard issues of gradient descent, for recurrent neural
network two additional problems have been identified: the vanishing gradient
and exploding gradient problems [Schmidhuber91], [Bengio94], [Pascanu13]. 

To understand these problem we can re-write the gradient computation as
follows:

.. figure:: images/rnn_formula.png
    :align: center
    :scale: 60

The issue is that the term :math:`\frac{\partial \mathbf{h}(t)}{\partial\mathbf{h}(t-k)}`, 
which is a product of :math:`k` matrices, in the same
way a product of :math:`k` numbers, can either grow to infinity or shrink to 0
exponentially fast with :math:`k`. 


Proposed solutions
------------------

Echo State Network (Reservoir Computing)
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

One proposed solution to this problem is to not train the recurrent 
weights :math:`W_{hh}` and input weights :math:`W_{uh}` [Jaeger01],
[Jaegre07].
One instead carefully initialize these weights.
Such initialization scheme usually look at the model size, the spectral
radius of the recurrent weight (which controls how quickly the model forgets
previously seen inputs) and input and target scaling and shifting, 

Additionally one can use leaky-integration neurons (low-pass filters on the
hidden units) to make different parts of the recurrent network focus on
different regions in frequency [Luskes] [Hermes]. 

Standard Recurrent Neural Networks
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

In [Sutskever13] a recurrent network is **initialized** very similar to an ESN before
training. The paper shows that careful initialization (together with a few
other elements like Nestorov momentum) can improve greatly the solutions one
gets from RNNs. 

Another approach to address the difficulty of training RNNs is to use **second
order methods** [Puskekios] [Martens].  


**Clipping the gradients** is another approach specifically designed to deal
with the exploding gradient problem [Pascanu13]. Each time the norm of the
gradient is larger than some threshold the gradients are rescaled such to
have their norm equal threshold. 

One can construct a **regularization term** that forces the gradient to preserve 
norm as they travel in time [Pascanu13]. Such an approach can address the
vanishing gradient problem.



Long-Short Term Memory (LSTM) models
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

In [Hochreiter01] proposes a solution to the vanishing gradient problem by
replacing the hidden units of the model with Long Short Term Memory Cells.
These cells contain a linear unit with a self recurrent weight of one 
which is guarded by an input and output
gate. The information is preserved inside a cell by closing the input gate.
Certain extensions of the basic model are usually used (for example by
considering a forget gate). 
A bi-directional version of the model (that goes both forward and backwards
in time) [Graves] and deep version [Graves] were also proposed.

Literature review 
-----------------

A (incomplete) list of papers on recurrent neural models:

 * in progress

