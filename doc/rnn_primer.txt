.. _rnn_primer:

Primer on Recurrent Neural Networks
===================================
.. Note::
    This tutorial assumes some familarity with recurrent neural models and
    it is meant more as a refresher on this model. At the end of the tutorial we provide a
    set of papers that could be used gain more insight into RNNs.

A recurrent neural network (RNN) is a recursive map, that at time step :math:`t` combines the state at
:math:`t-1` with the input to construct a new state. In order to formalize this sentence let us define
some notation:

  * :math:`u` stands for the input sequence. :math:`u(t)` is the frame at position :math:`t` in the sequence of inputs

  * :math:`h` stands for the (hidden) state of the model. :math:`h(t)` is the hidden state at step `t`

  * :math:`y` is the output of the model. :math:`y(t)` is the output at time step `t`

  * :math:`W_{hh}` is the recurrent weight matrix (that connects the hidden state :math:`h` to itself)

  * :math:`W_{uh}` is the input weight matrix that goes from the input :math:`u` to the hidden state :math:`h`

  * :math:`W_{hy}` is the ouput weight matrix that goes from the hidden state :math:`h` to the ouput layer :math:`y`

  * :math:`b_h` is the hidden state bias 

  * :math:`b_y` is the output layer bias

  * :math:`\sigma_h` is the activation function for the hidden state

  * :math:`\sigma_y` is the activation function for the output layer

The RNN is defined by the following two equations.

.. math::

    h(t) = f(u(t), h(t-1)) = \sigma_h(h(t-1)W_{hh} + u(t) W_{uh} + b_h)

    y(t) = \sigma_y(h(t) W_{hy} + b_y)

The recurrence is started 
from :math:`h_0` (the initial hidden state) which is provided by the practitioner 
or set to zero. At each time step we compute a new state :math:`h_t` and
predict an output for that step :math:`y_t`. Below this is represented as a
diagram:

.. figure:: images/rnn.png
    :align: center

.. Note::
    The RNN is similar to an MLP, with the exception that we allow
    connections among the hidden units. This changes drastically the model.
    Through these recurrent connections
    the model exhibits ''memory''. A RNN can approximate arbitrarly well any
    dynamical system, compared to a feed-forward neural model that can only
    approximate nonlinear functions.

The model can be used in several ways. One standard approach is to use 
the RNN to transcribe some input sequence into an output sequence. 
This means that the output sequence is of the same length as the input one, 
and at each time step the ouput of the RNN :math:`y(t)` is the prediction of
the :math:`t`-th frame of the target sequence. 

Another task could be to classify the possible input sequences into
different categories. One approach is to only consider the output at the end
of the sequence (after the model has seen the whole sequence). If the input 
sequence :math:`u` has :math:`T` steps, then :math:`y(T)` could be a softmax 
layer saying in which the category the current sequence falls. 

Training Recurrent Neural Networks
----------------------------------

Several algorithms are available for training recurrent neural networks. 
Real Time Recurrent Learning (RTRL), Atyia-Paros learning rule are just a 
few example. An interesting different approach which involves using the recurrent neural 
network to construct a random projection has also been proposed under the
name of Reservoir Computing.  Long Short Term Memory networks are recurrent 
neural networks that have a specific structure in the hidden layer which
helps them deal with long term correlations.

Backpropagation Through Time (BPTT)
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

BPTT is a widely used algorithm for computing the gradients of the recurrent
neural model. The idea is to unfold the model in time, resulting in a
potentially unbounded number of layers. In the figure below we show three
time steps corresponding to three layers (on the horizontal). :math:`C(t)`
stands for the cost at time step :math:`t`. 

.. figure:: images/unfolded.png
    :align: center

The green and blue arrors moving backwards in time shows the flow of the
gradient if one simply applies backpropagation on the unfolded graph. 

The following snipped of code sketches how one can implement efficiently
BPTT (note that before computing the gradients one has to do a forward pass
to compute all activations):

.. figure:: images/rnn_bptt.png
    :align: center



Difficulties of training recurrent models
-----------------------------------------

Besides the standard issues of gradient descent, for recurrent neural
network two additional problems have been identified: the vanishing gradient
and exploding gradient problems [Schmidhuber91], [Bengio94], [Pascanu13]. 

To understand these problem we can re-write the gradient computation as
follows:

.. figure:: images/rnn_formula.png

The issue is that the term XX which is a product of T matrices, in the same
way a product of T numbers, can either grow to infinity or shrink to 0
exponentially fast with T. 


Proposed solutions
------------------

Echo State Network (Reservoir Computing)
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
[HJ]
One proposed solution [Jaeger01],[Jaegre07] to this problem is to not train
the recurrent and input weights, but rather carefully initialize them.
Such initialization scheme usually look at the model size, the spectral
radius of the recurrent weight (which controls how quickly the model forgets
previously seen inputs) and input and target scaling and shifting, 

Additionally one can use leaky-integration neurons (low-pass filters on the
hidden units) to make different parts of the recurrent network focus on
different regions in frequency [..] [..]. 


Initialization
~~~~~~~~~~~~~~

In [Ilya] a recurrent network is initialized very similar to an ESN before
training. 

[Ilya]
* 10 lines paragraph saying trian BPTT starting from HJ initialization

Second-Order methods
~~~~~~~~~~~~~~~~~~~~
[Martens & Ilya/Puskorious with Kalman Filter]

* 10 lines paragraph saying 2nd order might work. Mention the key
components:
 - exact hessian. 

reason unclear

Clipping and regularizing the model
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
[Us]

Clipping
########
plot + the argument for exploding gradient

Regularizing the model
######################

Long-Short Term Memory (LSTM) models
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
[Schmidhuber/Graves]

Some extensions of the model 
----------------------------

leaky ESNs
~~~~~~~~~~
[cite HJ and the other guy]

bi-directional RNN/LSTM
~~~~~~~~~~~~~~~~~~~~~~~
[cite Alex Graves]

rectifier units RNN
~~~~~~~~~~~~~~~~~~~
[cite us]

deep LSTM
~~~~~~~~~
[cite Alex Graves]

